{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashivashankars/CMPE255_Assignments/blob/main/2_Unsloth's_LoRA_(PEFT)_(SmolLM2_135M).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6xm7kImUrju"
      },
      "source": [
        "##prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0_S8fkNUhoU"
      },
      "outputs": [],
      "source": [
        "!pip -q install unsloth transformers accelerate peft bitsandbytes datasets trl evaluate rouge-score\n",
        "!pip -q install flash-attn --no-build-isolation  # If wheel available for your Colab GPU\n",
        "\n",
        "import torch, os, json, random\n",
        "from datasets import load_dataset, Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB108uSzU-4E"
      },
      "source": [
        "##Full Fine-Tuning (FFT) a tiny model (SmolLM2-135M)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tu8pFj_VDZ2"
      },
      "source": [
        "#1) Pick the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVfLKEuJVChR"
      },
      "outputs": [],
      "source": [
        "BASE_MODEL = \"HuggingFaceTB/SmolLM2-135M\"   # tiny & quick\n",
        "# ALT examples:\n",
        "# BASE_MODEL = \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\"\n",
        "# BASE_MODEL = \"meta-llama/Llama-3.1-8B\"  # needs bigger GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0o4NAG8VKFw"
      },
      "source": [
        "#2) Use a small supervised dataset (chat/coding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyYPLkszVM9s"
      },
      "outputs": [],
      "source": [
        "# Load a simpler instruction-following dataset\n",
        "ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:500]\")\n",
        "\n",
        "def to_chat(example):\n",
        "    # Format the Alpaca dataset into a chat-like structure\n",
        "    # This is a simplified conversion; a proper chat template would be better\n",
        "    messages = []\n",
        "    if example[\"instruction\"]:\n",
        "        messages.append({\"role\": \"user\", \"content\": example[\"instruction\"]})\n",
        "    if example[\"input\"]:\n",
        "         messages.append({\"role\": \"user\", \"content\": example[\"input\"]}) # Append input as part of user message or a separate turn if appropriate for the template\n",
        "    if example[\"output\"]:\n",
        "        messages.append({\"role\": \"assistant\", \"content\": example[\"output\"]})\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "# Apply the formatting function and remove original columns\n",
        "train = ds.map(to_chat, remove_columns=[\"instruction\", \"input\", \"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utLLFKOBVTz5"
      },
      "source": [
        "#3) Load model for full fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcX31HfcVW9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de3b82d-61e9-413f-b1cd-9b0ba7da227a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "HuggingFaceTB/SmolLM2-135M does not have a padding token! Will use pad_token = <|endoftext|>.\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = 2048\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL,\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,         # full precision training\n",
        "    load_in_8bit = False,\n",
        "    dtype = torch.float16,\n",
        "    full_finetuning = False,       # <-- key for FFT\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IYNvlSuVazB"
      },
      "source": [
        "#4) Pack data and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b528764",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32bea65c-ac1e-4247-8443-ca1d588fd1b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokenized examples: 500\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 0) Pick ONE template that matches your model.\n",
        "# If you’re using Qwen/ChatML-style formatting, use this:\n",
        "CHATML_TEMPLATE = \"\"\"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{'<|im_start|>assistant\\n'}}{% endif %}\"\"\"\n",
        "\n",
        "# If your model is Llama 3 Instruct, use this instead:\n",
        "# LLAMA3_TEMPLATE = \"\"\"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n' + message['content'] + '<|eot_id|>'}}{% elif message['role'] == 'user' %}{{'<|start_header_id|>user<|end_header_id|>\\n' + message['content'] + '<|eot_id|>'}}{% elif message['role'] == 'assistant' %}{{'<|start_header_id|>assistant<|end_header_id|>\\n' + message['content'] + '<|eot_id|>'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{'<|start_header_id|>assistant<|end_header_id|>\\n'}}{% endif %}\"\"\"\n",
        "\n",
        "# If your model is Gemma Instruct, use this:\n",
        "# GEMMA_TEMPLATE = \"\"\"{% for message in messages %}{% if loop.first %}{{'<bos>'}}{% endif %}{{'<start_of_turn>' + message['role'] + '\\n' + message['content'] + '<end_of_turn>\\n'}}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\"\"\n",
        "\n",
        "# 1) SET the template once (choose the one that fits your model)\n",
        "tokenizer.chat_template = CHATML_TEMPLATE  # swap to LLAMA3_TEMPLATE or GEMMA_TEMPLATE if needed\n",
        "\n",
        "# (Optional) ensure special tokens exist if your tokenizer doesn’t know them\n",
        "# This is safe even if they already exist; it’s a no-op then.\n",
        "specials = {\"additional_special_tokens\": [\"<|im_start|>\", \"<|im_end|>\"]}\n",
        "try:\n",
        "    num_added = tokenizer.add_special_tokens(specials)\n",
        "    if num_added:\n",
        "        print(f\"Added {num_added} special tokens to tokenizer.\")\n",
        "        # If you have `model`, do: model.resize_token_embeddings(len(tokenizer))\n",
        "except Exception as _:\n",
        "    pass  # not critical; training can still proceed\n",
        "\n",
        "def _is_valid_messages(msgs):\n",
        "    if not isinstance(msgs, list) or not msgs:\n",
        "        return False\n",
        "    for m in msgs:\n",
        "        if not (isinstance(m, dict) and \"role\" in m and \"content\" in m and isinstance(m[\"content\"], str)):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def tokenize_chat(example):\n",
        "    msgs = example.get(\"messages\")\n",
        "    if not _is_valid_messages(msgs):\n",
        "        return {\"input_ids\": [], \"attention_mask\": []}\n",
        "\n",
        "    try:\n",
        "        # Render -> text (template comes from tokenizer.chat_template)\n",
        "        rendered = tokenizer.apply_chat_template(\n",
        "            msgs,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "        if not rendered.strip():\n",
        "            return {\"input_ids\": [], \"attention_mask\": []}\n",
        "\n",
        "        toks = tokenizer(rendered, add_special_tokens=False, return_attention_mask=True)\n",
        "        ids = toks.get(\"input_ids\", [])\n",
        "        attn = toks.get(\"attention_mask\", [])\n",
        "        return {\"input_ids\": ids, \"attention_mask\": attn} if ids else {\"input_ids\": [], \"attention_mask\": []}\n",
        "    except Exception as e:\n",
        "        print(\"Tokenization error:\", repr(e))\n",
        "        try:\n",
        "            print(\"First roles:\", [m.get(\"role\",\"?\") for m in msgs[:5]])\n",
        "        except:\n",
        "            pass\n",
        "        return {\"input_ids\": [], \"attention_mask\": []}\n",
        "\n",
        "# 2) Map → return python lists (Arrow-friendly), then filter non-empty rows\n",
        "train_tokenized = train.map(\n",
        "    tokenize_chat,\n",
        "    remove_columns=[c for c in train.column_names if c != \"messages\"]  # keep messages for debug until after filter\n",
        ")\n",
        "\n",
        "train_tokenized = train_tokenized.filter(lambda x: isinstance(x[\"input_ids\"], list) and len(x[\"input_ids\"]) > 0)\n",
        "\n",
        "# 3) Now drop 'messages' if you don’t need it further\n",
        "if \"messages\" in train_tokenized.column_names:\n",
        "    train_tokenized = train_tokenized.remove_columns([\"messages\"])\n",
        "\n",
        "print(f\"Number of tokenized examples: {len(train_tokenized)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a298d367"
      },
      "source": [
        "#5) Quick evaluation + inference demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd5KLRt7nMWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ad20fc-ec5b-4f12-93c0-f853d0e00e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.75}\n"
          ]
        }
      ],
      "source": [
        "# !pip install -q evaluate\n",
        "import evaluate\n",
        "\n",
        "# Example: accuracy\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "preds = [1, 0, 1, 1]\n",
        "refs  = [1, 0, 0, 1]\n",
        "print(metric.compute(predictions=preds, references=refs))  # {'accuracy': 0.75}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCFjw1czn1Hx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3323ed39-bdbe-446e-93f1-8f2649784545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "HuggingFaceTB/SmolLM2-135M-Instruct does not have a padding token! Will use pad_token = <|endoftext|>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel # Import FastLanguageModel\n",
        "from transformers import AutoTokenizer, pipeline # Keep AutoTokenizer\n",
        "\n",
        "MODEL_ID = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "\n",
        "# Use FastLanguageModel for loading to ensure Unsloth patching is applied\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_ID,\n",
        "    max_seq_length=max_seq_length, # Reusing max_seq_length from previous cell\n",
        "    dtype=torch.float16,\n",
        "    load_in_4bit=True, # Important for Unsloth models\n",
        "    full_finetuning=False, # Set to False for inference\n",
        ")\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "gen_kwargs = {\n",
        "    \"max_new_tokens\": 150,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "    \"return_full_text\": False\n",
        "}\n",
        "\n",
        "def generate(prompt):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    out = pipe(text, **gen_kwargs)[0][\"generated_text\"]\n",
        "    return out.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEWQy5-4qxYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "914a4d80-4a2b-4c4a-cfe6-e6d884d4f879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's how you can implement this in Python:\n",
            "```python\n",
            "class Node:\n",
            "    def __init__(self, data):\n",
            "        self.data = data\n",
            "        self.next = None\n",
            "\n",
            "\n",
            "def reverse_linked(head): \n",
            "    if head == None or tail is None: \n",
            "        return\n",
            "\n",
            "    # Create the new node and append it at the end of the original one \n",
            "    \n",
            "    # If there isn't enough space for both nodes then create them together \n",
            "  \n",
            "    # Reverse the link between these two nodes\n",
            "  \n",
            "  # Return the reversed linked list after reversing all elements from left side up until right edge \n",
            "\n",
            "  \n",
            " ```\u000b### Example Output ###\n",
            "Node1 -> Node2 -> Node3 -> Node4 → Node5-> Node6   \n",
            "`\n",
            "----\n",
            "A Python decorator is an expression that adds new behavior to existing functions or classes based on their attributes and methods. It's essentially a special kind of function where you're not just modifying the original code but also adding your own custom logic within it using decorators. This allows for more flexibility when building complex applications from scratch without having to modify every single line of code individually. Here's how we can create one:\n",
            "```python\n",
            "def my_decorator(func):\n",
            "    def wrapper(*args):\n",
            "        print(\"Something inside this decorator:\", func)\n",
            "        return args + [1] * len(args)\n",
            "\n",
            "    return wrapper()\n",
            "\n",
            "\n",
            "@my_decorator\n",
            "class MyClass():\n",
            "    pass\n",
            "\n",
            "\n",
            "# Example usage:\n",
            "MyClass().\n"
          ]
        }
      ],
      "source": [
        "print(generate(\n",
        "    \"Write a clean Python function to reverse a linked list iteratively. Only code.\"\n",
        "))\n",
        "print(\"----\")\n",
        "print(generate(\n",
        "    \"Explain what a Python decorator is in 3 sentences with a small example.\"\n",
        "))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}