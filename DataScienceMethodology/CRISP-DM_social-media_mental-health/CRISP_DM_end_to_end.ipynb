{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "CRISP-DM â€” Social Media & Mental Health"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# CRISP-DM â€” Social Media & Mental Health\n_Last updated: 2025-11-01 17:42_"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\n# Kaggle API setup (Colab-safe)\n\n1. Get your API key from Kaggle: Account â†’ **Create New API Token** â†’ downloads `kaggle.json`.\n2. In **Colab**: upload `kaggle.json` when prompted in the first cell below.\n\n"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "source": "\n#@title ðŸ”‘ Kaggle setup & dataset download\n# This cell works in Colab. If running locally, ensure kaggle is installed and KAGGLE_CONFIG_DIR is set.\nimport os, json, pathlib, zipfile, subprocess, sys\nfrom google.colab import files\n\nprint(\"Upload your kaggle.json (Kaggle â†’ Account â†’ Create New API Token).\")\nuploaded = files.upload()\nassert 'kaggle.json' in uploaded, \"Please upload kaggle.json\"\nos.makedirs('/root/.kaggle', exist_ok=True)\nwith open('/root/.kaggle/kaggle.json', 'wb') as f:\n    f.write(uploaded['kaggle.json'])\nos.chmod('/root/.kaggle/kaggle.json', 0o600)\n\n!pip -q install kaggle\n!kaggle datasets download -d ayeshaimran123/social-media-and-mental-health-balance -p data --force\nos.makedirs(\"data\", exist_ok=True)\n# Unzip all archives\nfor z in os.listdir('data'):\n    if z.endswith('.zip'):\n        import zipfile\n        with zipfile.ZipFile(os.path.join('data', z)) as zz:\n            zz.extractall('data')\nprint(\"âœ… Dataset downloaded to ./data\")\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Business Understanding\nDescribe objectives, KPIs, constraints, stakeholders, risks, hypotheses."
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "source": "# TODO: Define success metrics (e.g., F1, AUC) and business utility mapping.\nSUCCESS_METRIC = 'F1'",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Data Understanding\nProfile dataset: schema, missingness, distributions, correlations, target feasibility."
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "source": "\n# EDA starter\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nimport glob, os\n\nfiles = glob.glob('data/*.*')\nprint('Found data files:', files)\n\n# Try loading common names\ncandidates = [f for f in files if f.lower().endswith(('.csv','.tsv','.parquet','.xlsx'))]\nif not candidates:\n    raise SystemExit(\"Place the main CSV/TSV/XLSX/Parquet file in ./data\")\npath = candidates[0]\nprint(\"Loading:\", path)\nif path.endswith('.csv') or path.endswith('.tsv'):\n    df = pd.read_csv(path)\nelif path.endswith('.xlsx'):\n    df = pd.read_excel(path)\nelse:\n    df = pd.read_parquet(path)\n\nprint(df.shape); display(df.head())\ndisplay(df.describe(include='all'))\nmissing = df.isna().mean().sort_values(ascending=False).to_frame('missing_rate')\ndisplay(missing.head(20))\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Data Preparation\nTarget definition, leakage audit, splits, encoding, feature engineering."
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "source": "\n# Example: simple cleaning scaffold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\ndf_clean = df.copy()\n# TODO: set TARGET column name correctly\nTARGET = 'target'  # <-- change me after inspecting the dataset\n\nassert TARGET in df_clean.columns, \"Set TARGET to the correct label column\"\ny = df_clean[TARGET]\nX = df_clean.drop(columns=[TARGET])\n\nnum_cols = X.select_dtypes(include=['number']).columns.tolist()\ncat_cols = X.select_dtypes(exclude=['number']).columns.tolist()\n\nnumeric_proc = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\ncategorical_proc = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))\n])\npre = ColumnTransformer([\n    (\"num\", numeric_proc, num_cols),\n    (\"cat\", categorical_proc, cat_cols)\n])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y if y.nunique()<20 else None, random_state=42)\nprint(X_train.shape, X_test.shape)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Modeling\nBaselines, model selection, CV, hyperparameters."
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "source": "\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, roc_auc_score, f1_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\n\nmodel = Pipeline([(\"pre\", pre), (\"clf\", LogisticRegression(max_iter=200))])\nscores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"f1_macro\")\nprint(\"CV F1_macro:\", scores.mean(), \"+/-\", scores.std())\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\nprint(classification_report(y_test, preds))\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Evaluation\nHoldout integrity, error analysis, fairness, sensitivity."
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "source": "\nimport numpy as np\n# TODO: add confusion matrix, calibration plots, subgroup analysis\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Deployment\nPackaging plan, API or batch, monitoring & drift."
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form"
      },
      "source": "# TODO: Export pipeline with joblib, add model card, define monitoring metrics.",
      "outputs": [],
      "execution_count": null
    }
  ]
}