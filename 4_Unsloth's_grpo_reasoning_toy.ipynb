{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashivashankars/CMPE255_Assignments/blob/main/4_Unsloth's_grpo_reasoning_toy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BFXz5gebHPIB",
      "metadata": {
        "id": "BFXz5gebHPIB"
      },
      "source": [
        "# Colab 4 — GRPO Reasoning (DeepSeek‑style) with Unsloth\n",
        "\n",
        "**Goal:** Train short **GRPO** reasoning steps on a tiny math/logic set."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n9Btdv2mHPID",
      "metadata": {
        "id": "n9Btdv2mHPID"
      },
      "source": [
        "##Overview\n",
        "Teach the model to **show its work** on reasoning questions. GRPO gives reward to better chains‑of‑thought. We keep a tiny built‑in dataset so it always runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21Ja-Yx2HPID",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21Ja-Yx2HPID",
        "outputId": "a9d718c2-2ca5-4666-bdee-1ab3fd4c0e65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "!pip -q install --upgrade pip\n",
        "# Core libs\n",
        "!pip -q install \"unsloth>=2025.10.0\" \"transformers>=4.45.0\" \"datasets>=2.19.0\" \"accelerate>=1.0.0\" \"trl>=0.9.6\" \"peft>=0.13.0\" \"bitsandbytes>=0.44.0\" \"evaluate>=0.4.3\" \"scikit-learn>=1.5.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3g-pYpBrHPIE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g-pYpBrHPIE",
        "outputId": "ee9b1bfa-77ae-4a26-913c-31e0b6f95eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timestamp: 2025-11-09 05:40:39.516658\n",
            "Python: 3.12.12\n",
            "Torch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "Device: NVIDIA A100-SXM4-40GB\n",
            "Capability: (8, 0)\n"
          ]
        }
      ],
      "source": [
        "import os, random, numpy as np, torch, platform\n",
        "from datetime import datetime\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED);\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "print(\"Timestamp:\", datetime.now())\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
        "    print(\"Capability:\", torch.cuda.get_device_capability(0))\n",
        "else:\n",
        "    print(\"⚠️ GPU not found. Colab > Runtime > Change runtime type > GPU is recommended.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dg_MktvoHPIE",
      "metadata": {
        "id": "dg_MktvoHPIE"
      },
      "source": [
        "## Tiny inline reasoning dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JR5rHQFIHPIE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR5rHQFIHPIE",
        "outputId": "88c2db35-fc25-4066-856a-9885f9adc84f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json, os\n",
        "reasoning = [\n",
        "  {\"question\":\"If a pen costs $2 and a notebook costs twice as much, what is the total cost?\", \"answer\":\"$6\", \"rationale\":\"Notebook costs $4 (twice of $2). Total $2+$4=$6.\"},\n",
        "  {\"question\":\"What is 15% of 80?\", \"answer\":\"12\", \"rationale\":\"0.15*80=12.\"},\n",
        "  {\"question\":\"A train travels 60 km in 1.5 hours. What is its average speed?\", \"answer\":\"40 km/h\", \"rationale\":\"Speed=Distance/Time=60/1.5=40.\"},\n",
        "]\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "with open(\"data/grpo_tiny.jsonl\",\"w\") as f:\n",
        "    for r in reasoning: f.write(json.dumps(r)+\"\\n\")\n",
        "len(reasoning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B_GNm-gXHPIF",
      "metadata": {
        "id": "B_GNm-gXHPIF"
      },
      "source": [
        "## Load model (QLoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dgxvhHj9HPIF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgxvhHj9HPIF",
        "outputId": "38230e51-8c79-4123-a0dd-da7cfcab775b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() else None\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3.1-8b-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "# Explicitly set the chat template for Llama-3.1\n",
        "tokenizer.chat_template = \"<|begin_of_text|>{% for message in messages %}{% if message['role'] == 'user' %}<|start_header_id|>user<|end_header_id|>\\n{{ message['content'] | trim }}<|eot_id|>{% elif message['role'] == 'assistant' %}<|start_header_id|>assistant<|end_header_id|>\\n{{ message['content'] | trim }}<|eot_id|>{% elif message['role'] == 'system' %}<|start_header_id|>system<|end_header_id|>\\n{{ message['content'] | trim }}<|eot_id|>{% endif %}{% endfor %}{% if add_generation_prompt %}<|start_header_id|>assistant<|end_header_id|>\\n{% endif %}\"\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7pSlFVKnHPIF",
      "metadata": {
        "id": "7pSlFVKnHPIF"
      },
      "source": [
        "## Build a simple GRPO-style reward (exact match on final answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ISC21bOxHPIF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "9e8700b440124a9c83ef055a48671aee",
            "233d08ccbe5d492f8fbdc47fb29d0124",
            "8f9aab8f79104b31af348bfc3eefe112",
            "de179f1147284b0e9d7b3cebe253c94d",
            "3df9575d60ca41078ba0d2de51934bb5",
            "ea2356e48e87435e94fc171f0be0e350",
            "831905e04a4f4865aecd8790e95d45d6",
            "dbecb80acc76401eb9550a0a02be1a4d",
            "72f22564654b449f99a5af690bb578ea",
            "3e8533fb3d5c489d98ea3f9364b0caab",
            "e9b4ef57de564ef0a5fef0f4295ef15f"
          ]
        },
        "id": "ISC21bOxHPIF",
        "outputId": "cbb1e33a-e6a5-4cb1-cb36-2dda701e5454"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e8700b440124a9c83ef055a48671aee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer', 'rationale', 'messages'],\n",
              "    num_rows: 3\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json, torch, random\n",
        "from datasets import Dataset\n",
        "ds = Dataset.from_list(reasoning)\n",
        "\n",
        "# Simple reward: 1.0 if final boxed answer matches, else 0.0\n",
        "def format_messages(ex):\n",
        "    prompt = f\"Solve step by step, then give final answer on a new line as 'Final: {ex['answer']}'.\\nQuestion: {ex['question']}\"\n",
        "    return [{\"role\":\"user\",\"content\":prompt}]\n",
        "ds = ds.map(lambda ex: {\"messages\": format_messages(ex)})\n",
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "roIeaC-JHPIF",
      "metadata": {
        "id": "roIeaC-JHPIF"
      },
      "source": [
        "## GRPO training loop (minimal toy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MJVqsNtyHPIF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJVqsNtyHPIF",
        "outputId": "d543e7aa-2dae-40ae-dc86-9c6b50d667c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0 reward=0.00 sample:\n",
            "user\n",
            "Solve step by step, then give final answer on a new line as 'Final: 40 km/h'.\n",
            "Question: A train travels 60 km in 1.5 hours. What is its average speed?assistant\n",
            "Average speed =...\n",
            "\n",
            "step 1 reward=1.00 sample:\n",
            "user\n",
            "Solve step by step, then give final answer on a new line as 'Final: 12'.\n",
            "Question: What is 15% of 80?assistant\n",
            "What is 25% of 80?िरफ\n",
            "What is 10% of 80?िरफ\n",
            "What is 50% of 80?िर...\n",
            "\n",
            "step 2 reward=1.00 sample:\n",
            "user\n",
            "Solve step by step, then give final answer on a new line as 'Final: $6'.\n",
            "Question: If a pen costs $2 and a notebook costs twice as much, what is the total cost?assistant\n",
            "Pleas...\n",
            "\n",
            "Toy GRPO loop done (for full GRPO use the official tutorial).\n"
          ]
        }
      ],
      "source": [
        "import torch, math\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "def rollout_and_reward(messages):\n",
        "    x = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
        "    y = model.generate(x, max_new_tokens=96, do_sample=True, temperature=0.8, top_p=0.95, output_scores=True, return_dict_in_generate=True)\n",
        "    text = tokenizer.decode(y.sequences[0], skip_special_tokens=True)\n",
        "    reward = 1.0 if \"Final: \" in text and text.strip().split(\"Final:\")[-1].strip().startswith(messages[0][\"content\"].split(\"Final:\")[-1].strip()) else 0.0\n",
        "    return y, reward, text\n",
        "\n",
        "for step, ex in enumerate(ds.shuffle(seed=42).select(range(len(ds)))):\n",
        "    y, reward, text = rollout_and_reward(ex[\"messages\"])\n",
        "    # Fake GRPO: encourage higher logprobs when reward=1, otherwise small penalty\n",
        "    loss = (1.0 - reward) * 0.1\n",
        "    loss = torch.tensor(loss, requires_grad=True, device=model.device)\n",
        "    loss.backward()\n",
        "    if (step+1)%2==0:\n",
        "        optim.step(); optim.zero_grad()\n",
        "    print(f\"step {step} reward={reward:.2f} sample:\\n{text[:180]}...\\n\")\n",
        "print(\"Toy GRPO loop done (for full GRPO use the official tutorial).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RodRTDUcHPIG",
      "metadata": {
        "id": "RodRTDUcHPIG"
      },
      "source": [
        "## Quick check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X8cmpUFRHPIG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8cmpUFRHPIG",
        "outputId": "825c0327-0e9a-48e7-8287-2f2ade9dab41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user\n",
            "Solve step by step and end with 'Final: X'.\n",
            "Question: What is 20% of 90?assistant\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of\n"
          ]
        }
      ],
      "source": [
        "def ask(q):\n",
        "    msgs=[{\"role\":\"user\",\"content\":f\"Solve step by step and end with 'Final: X'.\\nQuestion: {q}\"}]\n",
        "    x = tokenizer.apply_chat_template(msgs, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
        "    y = model.generate(x, max_new_tokens=120, do_sample=False)\n",
        "    print(tokenizer.decode(y[0], skip_special_tokens=True))\n",
        "ask(\"What is 20% of 90?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "htJbIqNiHPIG",
      "metadata": {
        "id": "htJbIqNiHPIG"
      },
      "source": [
        "## Save adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I5zcAqiVHPIG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5zcAqiVHPIG",
        "outputId": "15a2e204-3036-42b0-856f-535a9c2c4c7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to llama31_grpo_toy_adapter\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(\"llama31_grpo_toy_adapter\")\n",
        "tokenizer.save_pretrained(\"llama31_grpo_toy_adapter\")\n",
        "print(\"Saved to llama31_grpo_toy_adapter\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}