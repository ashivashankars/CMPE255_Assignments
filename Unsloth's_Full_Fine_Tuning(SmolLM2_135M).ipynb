{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashivashankars/CMPE255_Assignments/blob/main/Unsloth's_Full_Fine_Tuning(SmolLM2_135M).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6xm7kImUrju"
      },
      "source": [
        "##prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0_S8fkNUhoU",
        "outputId": "4a424993-ec2a-4bc6-8fde-0ed7387df644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/61.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m351.3/351.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "========\n",
            "Switching to PyTorch attention since your Xformers is broken.\n",
            "========\n",
            "\n",
            "Requires Flash-Attention version >=2.7.1,<=2.8.2 but got 2.8.3.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "!pip -q install unsloth transformers accelerate peft bitsandbytes datasets trl evaluate rouge-score\n",
        "!pip -q install flash-attn --no-build-isolation  # If wheel available for your Colab GPU\n",
        "\n",
        "import torch, os, json, random\n",
        "from datasets import load_dataset, Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB108uSzU-4E"
      },
      "source": [
        "##Full Fine-Tuning (FFT) a tiny model (SmolLM2-135M)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tu8pFj_VDZ2"
      },
      "source": [
        "#1) Pick the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVfLKEuJVChR"
      },
      "outputs": [],
      "source": [
        "BASE_MODEL = \"HuggingFaceTB/SmolLM2-135M\"   # tiny & quick\n",
        "# ALT examples:\n",
        "# BASE_MODEL = \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\"\n",
        "# BASE_MODEL = \"meta-llama/Llama-3.1-8B\"  # needs bigger GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0o4NAG8VKFw"
      },
      "source": [
        "#2) Use a small supervised dataset (chat/coding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "1120250b8d5040f6be77ec20f67774b6",
            "40830b6e6c6047cc8c672366f0854d2d",
            "24dfb9a7ea8c42649a9d032937c66293",
            "7e681f5ff699459f9c617bdf7e30e911",
            "d5a9789367fa4b0bbfcdba2f142d8196",
            "96a73e189f5540b2aa21b40681efd1a1",
            "5321e60f11eb4721a087c632e787bbb8",
            "1c75515129bb4be39c93bdcc731e18ee",
            "9af5fd145ef04121aa26ea9f4d6f0d78",
            "3bbc0689a7544ce4a5b512e20a801aaa",
            "d703cd94e34f4b3ba3ecca4516b1d44b",
            "c6d44337dc744e54b23bee3de0192448",
            "ecde412e276e4da78efb4500f02dcd45",
            "c75126859ffc41b6af6d8fc0112842d0",
            "56ed6cdfb0d44b898f3a676a562cf372",
            "ef52474d0df64d9f896f864393d67b97",
            "d7a886a10ce14ec4a81ca6123ebf85d4",
            "3ad55ec9eb7841a286db804d98e49a71",
            "d188d8c3eea54a4daf43ed78e48dc1e9",
            "69907e4861ca4e368354fe06ede2572d",
            "97f71d24fb7649db87ed2dc817db45fa",
            "395b0162746a4f27bde1091c8648bd7d",
            "550b50358ff2465d9a9ba4345f13b387",
            "d733e2daa3e04a4e8d249aaf3d3fc346",
            "70d2acaabee84d37855a6284f360c077",
            "690069e32398435e8571c68ad566f87e",
            "1bfc0739fbf64d8ba562987194609aa0",
            "8033839f483e4410b58311e2338cba04",
            "09ce703b8fdc4fca9011d73d7fe24687",
            "640a8297a850478b8a56cfa02a5a0990",
            "68795a40f9ed4d54a77b41540d6e9926",
            "fd20b9cdc3e9431d99bb175ce2af30d4",
            "a7d54e420351474baa1433c5fd35e6c1",
            "cafd30bc928e417bb4c51231336140c7",
            "9f492e3fe58f422387e53c3b19f001a7",
            "e4bf814bdde0484fa7fd7bffdb13fded",
            "4efc6868479d462fbce2d281a800cecd",
            "0b4596d161fe4e5387f2b53a7bcb31ec",
            "1e7b480bc3ac4793b7b20b65a447b4d1",
            "eb6dfd06cc864ca8bc1fe229c6474f03",
            "201394ec0dfc47fd9392c42809eb07e9",
            "fb4c93ee8862421bb6a901bba2ff402d",
            "aa1292aec53f4c7d97fddb5a5466b772",
            "a9d913f1b4b741cfb438e783f6a71c87"
          ]
        },
        "id": "nyYPLkszVM9s",
        "outputId": "2c2d5794-de3e-4fe6-dec8-c7a5a59da9ed"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1120250b8d5040f6be77ec20f67774b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6d44337dc744e54b23bee3de0192448",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001-a09b74b3ef9c3b(â€¦):   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "550b50358ff2465d9a9ba4345f13b387",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cafd30bc928e417bb4c51231336140c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load a simpler instruction-following dataset\n",
        "ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:500]\")\n",
        "\n",
        "def to_chat(example):\n",
        "    # Format the Alpaca dataset into a chat-like structure\n",
        "    # This is a simplified conversion; a proper chat template would be better\n",
        "    messages = []\n",
        "    if example[\"instruction\"]:\n",
        "        messages.append({\"role\": \"user\", \"content\": example[\"instruction\"]})\n",
        "    if example[\"input\"]:\n",
        "         messages.append({\"role\": \"user\", \"content\": example[\"input\"]}) # Append input as part of user message or a separate turn if appropriate for the template\n",
        "    if example[\"output\"]:\n",
        "        messages.append({\"role\": \"assistant\", \"content\": example[\"output\"]})\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "# Apply the formatting function and remove original columns\n",
        "train = ds.map(to_chat, remove_columns=[\"instruction\", \"input\", \"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utLLFKOBVTz5"
      },
      "source": [
        "#3) Load model for full fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "bd71141e40d74b68bb2045daa731606c",
            "40fc55b1b68146788cdb55c8512004ab",
            "d6baf9e3dc9944f89769e8274c767fc7",
            "9f33a1c416544fbfb0981a3296661950",
            "a4418fc749f547eba183fc7532d9fe9e",
            "cd507e08d2fb40d7bd6e883b32995318",
            "39e3ab688fa94aac83369fc20e0786b5",
            "941797da8ace4021b9d7d79971329078",
            "3182958f516b46c19edd4426111bf8c9",
            "dba14c9bdac34bcc9e24135ebb096298",
            "679b5ee749f64e4682d3df3fc15bba03",
            "98a89d424d094ebd9f3161ad01f4d443",
            "de8b7405fcf84a05b8f5bbdea49718fc",
            "1df0c016754b4e9f935ef42e45a3b161",
            "40dd5b19dd7f4a409abdcf70c2d9e45a",
            "44a8737ba0e143bbaa10b36b0bea7fba",
            "5fe7fec1c9ef4b8aa45868bf62a1e482",
            "ed64b064951b442aaa9976a74007ba2f",
            "674d5a651f56463e87c9f619698c9698",
            "8d3a5dfe751f491c9bbda40036df18f7",
            "01c63fe48aa346a7a4aef533c97f8198",
            "3a7bf926b9e64e2eb72dad79e2477d3c",
            "dd3d0520ba9144d5a17ca17d2eaa301f",
            "a510fe597bf849a585b93c8bbe844de9",
            "1737893a4ecc45bda2e390ba841d2aef",
            "847e9c51dcb04bee96966eb910ae5cf3",
            "778406653603495eaa3caa80a5148340",
            "338436b7acd846ea87f2f39bc8e01e84",
            "a33aa2b32e364ebd9d70817d03a422d9",
            "1aec9cb935b14f208ac69d8a4d827df4",
            "6b233944eb3d4c70a972e76f37e20303",
            "93c5a176b21a460aaeb0bd4adc47d67b",
            "d4713703a7814f1f8c3416d24711f992",
            "3a0df29ab4fd489eb5c830da0a8c6242",
            "5af9e7e3e1064275b5f59b2133d1cd70",
            "120db23ae16940869885ac9e872e7182",
            "39ecd6c6b4fa4d94be055deeb64bc28d",
            "2e72a861b6da4b9f8d7494695799b38f",
            "f2a64d551969457fa92085c2ae99ab1f",
            "8c9e4e27e0df4d469aa9b9cc3efde87d",
            "71d2bbbda4a44c78aaa0056c67f5d49f",
            "509b868899b040a5bb3f7a5ed545c423",
            "ff3a70d2e6624b7ebb09b8d60431f914",
            "7af66b5db19848c88616ebf9e10d4449",
            "dd30772979bc4e02b5767ca58d86b006",
            "8abb9f13b8504630b238b48be0e3d6c7",
            "6060cb9af3a74c13928969063074e70f",
            "f9ad7c7441ab4e42b2a5b115c94f65d0",
            "11c51ad49cc64bff8dfcc44a26d170a8",
            "391c7c87d8d740e0b5ea44081972e54b",
            "74023388f6ed4fb6b8cc92602ea12697",
            "7ba8a7e4a253410686bda7529a3232ea",
            "4014174ef3454df8b0152573987d03cd",
            "028172b07ae34d78b2cfc7a1151043d0",
            "bf72621dde564ba695a87036853c80d3",
            "10aa4de3d84b41729c88a62af140ac39",
            "ce4609f2e40949659328bd2420384252",
            "f698eab63e1a4264992dd472df4f42a7",
            "069dcc64fa994b6d9ed8060c0c913cc2",
            "56e7021196054651ab3310dd87517e50",
            "e248bc81c18748d785625573e1a725dd",
            "fb14131bfd704492a51b1da7dd7c46b5",
            "5f5ed9a3d1ed45619245194172932863",
            "d2250467c878467291ea2be943451f98",
            "c727d0b3e6e941f3a5c698a522e9c0ed",
            "e631237119df4fceb376d9c909726d65",
            "451cefc3e5e24cda8efe7844449c9752",
            "59160682f0eb4df0aeb582c8d2661a3e",
            "85800e2c2dc347c3832bc78c2dadcc8b",
            "8e1219dc42c24bce9660e30d4870460b",
            "7179b24babfe4ee4a697467d4568bbfa",
            "e06411bd179e48138a631b656e178408",
            "87fe9f958ef14482ba0d6565431ae694",
            "870abe6e313e40bca1c3d9688b3c9def",
            "cf5d47d9d7304128a7563f8273043586",
            "5ec317730b134decbd60712039a8a4d3",
            "e11d0b68dadd4ec58aba573fd749ad5f"
          ]
        },
        "id": "bcX31HfcVW9_",
        "outputId": "de7c4552-b135-4dc6-c3ee-a3a6ada2e8b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Float16 full finetuning uses more memory since we upcast weights to float32.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd71141e40d74b68bb2045daa731606c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98a89d424d094ebd9f3161ad01f4d443",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd3d0520ba9144d5a17ca17d2eaa301f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a0df29ab4fd489eb5c830da0a8c6242",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd30772979bc4e02b5767ca58d86b006",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10aa4de3d84b41729c88a62af140ac39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "451cefc3e5e24cda8efe7844449c9752",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HuggingFaceTB/SmolLM2-135M does not have a padding token! Will use pad_token = <|endoftext|>.\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = 2048\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL,\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False,         # full precision training\n",
        "    load_in_8bit = False,\n",
        "    dtype = torch.float16,\n",
        "    full_finetuning = True,       # <-- key for FFT\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IYNvlSuVazB"
      },
      "source": [
        "#4) Pack data and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "130511bce56346318fd32921ffe7d9e4",
            "8c30cd1cc1a1449ba0ee7613a4938969",
            "cf252888861c43619f2783eb97a62a33",
            "121a713e2a1248f6bafde7d449220ecf",
            "ef599e708f4d49adb2f8696d521e969c",
            "af928e4681234ce5bf253f5b127d4c77",
            "8ceb45cbc26448eeb50ec29f0d64163b",
            "0dc8087ab00a45a6a36a62b63a2df5d9",
            "d7bea0bb561d4eabaeddafb6905132ff",
            "8f68fc54db2c4884b1f92230fdc55b38",
            "38efe636a1b84e1288bce9cde2ba33d9",
            "54b298916a8d494c8b7be13b2442bb9c",
            "cd9902d713cc4c1b93115c0e771e6962",
            "b1f32225f9844892820bcd79e6d48cd1",
            "b7b9d43aea8b42bbb62be77824e64b42",
            "2d676858dc0443508f28fa27752aa129",
            "3889c56c408b4bbfad2e85b7c44024fa",
            "1a1ae5920af74510ac031815101af8dd",
            "9e3149ab25b84d7d8761f860f18aab66",
            "9e99df46bbbf44a0a71f08433aa56c76",
            "8963ab1e494940a687eb118e56b87223",
            "a2d63a3355cd40589d3e546537768501"
          ]
        },
        "id": "8b528764",
        "outputId": "585c4e93-8c2f-4431-a552-125f9e91e417"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "130511bce56346318fd32921ffe7d9e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54b298916a8d494c8b7be13b2442bb9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokenized examples: 500\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 0) Pick ONE template that matches your model.\n",
        "# If youâ€™re using Qwen/ChatML-style formatting, use this:\n",
        "CHATML_TEMPLATE = \"\"\"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{'<|im_start|>assistant\\n'}}{% endif %}\"\"\"\n",
        "\n",
        "# If your model is Llama 3 Instruct, use this instead:\n",
        "# LLAMA3_TEMPLATE = \"\"\"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n' + message['content'] + '<|eot_id|>'}}{% elif message['role'] == 'user' %}{{'<|start_header_id|>user<|end_header_id|>\\n' + message['content'] + '<|eot_id|>'}}{% elif message['role'] == 'assistant' %}{{'<|start_header_id|>assistant<|end_header_id|>\\n' + message['content'] + '<|eot_id|>'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{'<|start_header_id|>assistant<|end_header_id|>\\n'}}{% endif %}\"\"\"\n",
        "\n",
        "# If your model is Gemma Instruct, use this:\n",
        "# GEMMA_TEMPLATE = \"\"\"{% for message in messages %}{% if loop.first %}{{'<bos>'}}{% endif %}{{'<start_of_turn>' + message['role'] + '\\n' + message['content'] + '<end_of_turn>\\n'}}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\"\"\n",
        "\n",
        "# 1) SET the template once (choose the one that fits your model)\n",
        "tokenizer.chat_template = CHATML_TEMPLATE  # swap to LLAMA3_TEMPLATE or GEMMA_TEMPLATE if needed\n",
        "\n",
        "# (Optional) ensure special tokens exist if your tokenizer doesnâ€™t know them\n",
        "# This is safe even if they already exist; itâ€™s a no-op then.\n",
        "specials = {\"additional_special_tokens\": [\"<|im_start|>\", \"<|im_end|>\"]}\n",
        "try:\n",
        "    num_added = tokenizer.add_special_tokens(specials)\n",
        "    if num_added:\n",
        "        print(f\"Added {num_added} special tokens to tokenizer.\")\n",
        "        # If you have `model`, do: model.resize_token_embeddings(len(tokenizer))\n",
        "except Exception as _:\n",
        "    pass  # not critical; training can still proceed\n",
        "\n",
        "def _is_valid_messages(msgs):\n",
        "    if not isinstance(msgs, list) or not msgs:\n",
        "        return False\n",
        "    for m in msgs:\n",
        "        if not (isinstance(m, dict) and \"role\" in m and \"content\" in m and isinstance(m[\"content\"], str)):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def tokenize_chat(example):\n",
        "    msgs = example.get(\"messages\")\n",
        "    if not _is_valid_messages(msgs):\n",
        "        return {\"input_ids\": [], \"attention_mask\": []}\n",
        "\n",
        "    try:\n",
        "        # Render -> text (template comes from tokenizer.chat_template)\n",
        "        rendered = tokenizer.apply_chat_template(\n",
        "            msgs,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "        if not rendered.strip():\n",
        "            return {\"input_ids\": [], \"attention_mask\": []}\n",
        "\n",
        "        toks = tokenizer(rendered, add_special_tokens=False, return_attention_mask=True)\n",
        "        ids = toks.get(\"input_ids\", [])\n",
        "        attn = toks.get(\"attention_mask\", [])\n",
        "        return {\"input_ids\": ids, \"attention_mask\": attn} if ids else {\"input_ids\": [], \"attention_mask\": []}\n",
        "    except Exception as e:\n",
        "        print(\"Tokenization error:\", repr(e))\n",
        "        try:\n",
        "            print(\"First roles:\", [m.get(\"role\",\"?\") for m in msgs[:5]])\n",
        "        except:\n",
        "            pass\n",
        "        return {\"input_ids\": [], \"attention_mask\": []}\n",
        "\n",
        "# 2) Map â†’ return python lists (Arrow-friendly), then filter non-empty rows\n",
        "train_tokenized = train.map(\n",
        "    tokenize_chat,\n",
        "    remove_columns=[c for c in train.column_names if c != \"messages\"]  # keep messages for debug until after filter\n",
        ")\n",
        "\n",
        "train_tokenized = train_tokenized.filter(lambda x: isinstance(x[\"input_ids\"], list) and len(x[\"input_ids\"]) > 0)\n",
        "\n",
        "# 3) Now drop 'messages' if you donâ€™t need it further\n",
        "if \"messages\" in train_tokenized.column_names:\n",
        "    train_tokenized = train_tokenized.remove_columns([\"messages\"])\n",
        "\n",
        "print(f\"Number of tokenized examples: {len(train_tokenized)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a298d367"
      },
      "source": [
        "#5) Quick evaluation + inference demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "4466f666d68f4f95921647d48426029a",
            "7925a4bc553b4a0ea71341f4a8215320",
            "a4ce309f4bb7478b82e4f01667bc15a3",
            "d1aa9dea966c43b0814d5f07005a1449",
            "b571a9fec48d41dd897f62a65b53a1d3",
            "51beafae164b47e3a1d9f014ac582ee9",
            "cc0d6eef7ac64154a182a306d740478a",
            "76c5ad86a1a54b3a8e5411064a3e6617",
            "21f063e34ed4440f96075636ecdebbd7",
            "3961ad38414c445dbc78ac511c8c69b5",
            "a4d2b048725b4847ac2d2ce80b5f93ef"
          ]
        },
        "id": "kd5KLRt7nMWp",
        "outputId": "57bb22d2-5be7-4ad3-8fbe-71e7df435fc4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4466f666d68f4f95921647d48426029a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'accuracy': 0.75}\n"
          ]
        }
      ],
      "source": [
        "# !pip install -q evaluate\n",
        "import evaluate\n",
        "\n",
        "# Example: accuracy\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "preds = [1, 0, 1, 1]\n",
        "refs  = [1, 0, 0, 1]\n",
        "print(metric.compute(predictions=preds, references=refs))  # {'accuracy': 0.75}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308,
          "referenced_widgets": [
            "a057bed9af09428294898e838c72ca15",
            "da04295842a941e68631e8e6ad1f0b2e",
            "76d5ef090653497fa037c48052a9f768",
            "76baf05bdc1d4305aa54d148839564a7",
            "c601280302c14eb698db29724f4ceed0",
            "ce9d45be958c493d81bd22a1521b8a09",
            "5f28546889a64c53a6926756226e9bee",
            "f2c4c193697b479ab100d84b3826685a",
            "d93e5856a72544c6a1ca8f728252ea1d",
            "591498f756694c74b53cc1a87efb2ee3",
            "03909bb859d842b1bc7dd57b17cef32b",
            "c2e6efec973240a387d678f843d07c7d",
            "e8211d56bc0e472c85c329c379f2918d",
            "6603d156670a4e49a71af07a4a754956",
            "ba915d5431e94bffa904a73cd4312b38",
            "3928529a497340b2867b6d4b202dbbc3",
            "1b1bcdeed033406883e8e5a34880f175",
            "599f07de4b944578b75b3b6cc4c897c9",
            "5560837d4001445292a3a8d3fc5fc0e0",
            "f245322fe4c04cf8934f4966962e96fa",
            "735a45d700c640f7933fbb651f0a3683",
            "3f818ad5a296485fa8479ae9c8e92891",
            "f63f95a66d924bcebdb3a614cb776af7",
            "f98991f805eb4cf9b51819a101c265f9",
            "f368fa92219642f69c62074c645abd95",
            "a472164ea539456b9fede0606153289a",
            "d31d509b504a4902882d4f928e0da6d8",
            "5d49eca5b5b34b3282f9ccc544f59e50",
            "0bc80ef328274beb8b0696c2ea91683a",
            "3c59ca0b295f4640b89f66d426d66918",
            "3fde65b406d444a3a17f9dd41f0b863d",
            "bf3de9a6d31146178c1b4e90a968a07d",
            "f2d3fe9d58074be29c21df68ca753235",
            "8c945a576ecf45248062ace2a92ddb82",
            "6b83866936114cb79d0c675ef6deeb48",
            "e48ca74612d14de794b6208542c80161",
            "3495718af239437883ab718166e32177",
            "5554453304eb485dab0bc8fed624e23c",
            "c1704fed183a43b48c5a494111057526",
            "2ae8bc8aca964fb9a051a6b2fb1fc0ea",
            "519368f53c214a67bd69a573259be8b8",
            "c19aede25a7940fbbe37a46912c11d8f",
            "5f2a57b340224a239b3a7bec8c8c5097",
            "972014529d8148cd8a0c17a8f5fdaf3f",
            "d09912d55694430e933c44ac0c9fffeb",
            "9561599e38de465c8d283b824ac936e3",
            "e77a49de75ae4a0e9d6d4f993cbac6f1",
            "bd99e53ba75342eeb3ecc6db51181d63",
            "03ae2a5b4b3b461fa574bfb19ea536af",
            "05825845333b44ac9723f4af88635a6f",
            "d3ce3268dda04c7999eb993337ec2d78",
            "3e20fb7eb9e94ed8baca32cd34b10828",
            "499207a2bec44ac9b31d76b27778223a",
            "b797ffb709734ab68e5c14229c202d33",
            "3deb34d288ef4ad3a11593682ac1d536",
            "545dfc5557b4434db3325b0df12a5a29",
            "402e57afdec1497089d5faf7dbf40e03",
            "ebd2d12efadb4b9c9cf3acf766137dc4",
            "ad89a915a519443489e269a0dae9b20b",
            "5e05146823e546bb91f0d21cfadc6931",
            "bb40db94eb0c4923ad0b502f4e722d69",
            "aef64ee208964d7197784f105eff4b21",
            "f95dee949d7748af9f634acdc191391c",
            "15f3a6e84e2f4d3791cf602bdf936d0d",
            "88a8267cd1d64215b777d861e4ccc9e4",
            "809d3ff4160a42dd82a203fa7b927dd1",
            "286e836193c8479e8d09c7832f14ce72",
            "93a880f863fd43f8a3b07d9cb4fbeec4",
            "2ed8e21222cf4abfaacb90890681f202",
            "b977becf096942e0bdbba5c16290070e",
            "952f504d536c4bd18b8c80b83e39170f",
            "2543fb13433f4f04b30a5be28146ba40",
            "51dc09f847e44e5fa50f33ce9af0f09c",
            "19cfc878cd044c27bf57fbb3c2facc85",
            "f1a15ffb86f44420a0b17d7ca4f31345",
            "ea63156ed63d4a78a1784096541d4078",
            "896cd6b2b86148c9930110705efe02ab",
            "0368764633d44eadbf0019276228038b",
            "384491aa05694a6882bdfed9a94c5633",
            "615c127c2ebc4f3791a60e53e41d98d6",
            "40b1fa6b1aa14ee395887121d2b98bdc",
            "cfb1fec7c5b442b689c4521e03c09665",
            "c2b73f26d38b4a318c24d568323cc215",
            "b68d15842b8644b89b8c73d596e07d2b",
            "a15032e901d84a2a95f567503c32a007",
            "cdd2f17f5dfa46bc8a8f0a40e669b08c",
            "8eed2141766648b087f0baabe838499c",
            "829ec6bc8afc4d26a7f0afc164e37ad6"
          ]
        },
        "id": "vCFjw1czn1Hx",
        "outputId": "c64527a9-9bf9-460d-b006-14299d250241"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a057bed9af09428294898e838c72ca15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2e6efec973240a387d678f843d07c7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f63f95a66d924bcebdb3a614cb776af7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c945a576ecf45248062ace2a92ddb82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d09912d55694430e933c44ac0c9fffeb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "545dfc5557b4434db3325b0df12a5a29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "286e836193c8479e8d09c7832f14ce72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0368764633d44eadbf0019276228038b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "MODEL_ID = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "gen_kwargs = {\n",
        "    \"max_new_tokens\": 150,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "    \"return_full_text\": False\n",
        "}\n",
        "\n",
        "def generate(prompt):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    out = pipe(text, **gen_kwargs)[0][\"generated_text\"]\n",
        "    return out.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEWQy5-4qxYm",
        "outputId": "28c114e6-d7ab-4bf2-9906-0664362afca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the implementation of the `reverse_linked_list` function in Python:\n",
            "```python\n",
            "class Node:\n",
            "    def __init__(self, data=None):\n",
            "        self.data = data\n",
            "        self.next = None\n",
            "\n",
            "\n",
            "def reverse_linked_list(head):\n",
            "    prev = None\n",
            "\n",
            "    while head!= None and head.next!= None:\n",
            "        next_node = head.next\n",
            "\n",
            "        if not prev or (prev == 0) and (not next_node else node1 for node2 in reversed((next_node).next)):\n",
            "            break\n",
            "        elif len > 3 * maxlen - 4 + minLen-5 : #max length minus 6 so that we can handle edge cases like last element\n",
            "----\n",
            "A Python decorator is an object that defines a set of methods or properties to be decorated for use within another class or module. It's used to add additional functionality and flexibility without modifying the original codebase. For instance:\n",
            "```python\n",
            "@decorator_name(method1) # This will decorate method2 as 'add' instead of just \"lambda\" (which would not work here).\n",
            "def myfunc():\n",
            "    pass\n",
            "myfunction = @wraparound(\"call\")()\n",
            "print(f\"{type(myfunction).__name__} called.\")\n",
            "# Outputs:\n",
            "// Calling `wrapper` function does nothing because it was already wrapped inside `functools`. However, if you want to call this wrapper directly like\n"
          ]
        }
      ],
      "source": [
        "print(generate(\n",
        "    \"Write a clean Python function to reverse a linked list iteratively. Only code.\"\n",
        "))\n",
        "print(\"----\")\n",
        "print(generate(\n",
        "    \"Explain what a Python decorator is in 3 sentences with a small example.\"\n",
        "))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}