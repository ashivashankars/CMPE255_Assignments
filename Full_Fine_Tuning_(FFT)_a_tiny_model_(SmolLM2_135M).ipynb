{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashivashankars/CMPE255_Assignments/blob/main/Full_Fine_Tuning_(FFT)_a_tiny_model_(SmolLM2_135M).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6xm7kImUrju"
      },
      "source": [
        "##prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0_S8fkNUhoU",
        "outputId": "32a4a739-29c3-4153-82e4-dd9c7afbb1cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/61.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m351.3/351.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "========\n",
            "Switching to PyTorch attention since your Xformers is broken.\n",
            "========\n",
            "\n",
            "Requires Flash-Attention version >=2.7.1,<=2.8.2 but got 2.8.3.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "!pip -q install unsloth transformers accelerate peft bitsandbytes datasets trl evaluate rouge-score\n",
        "!pip -q install flash-attn --no-build-isolation  # If wheel available for your Colab GPU\n",
        "\n",
        "import torch, os, json, random\n",
        "from datasets import load_dataset, Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB108uSzU-4E"
      },
      "source": [
        "##Full Fine-Tuning (FFT) a tiny model (SmolLM2-135M)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tu8pFj_VDZ2"
      },
      "source": [
        "#1) Pick the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVfLKEuJVChR"
      },
      "outputs": [],
      "source": [
        "BASE_MODEL = \"HuggingFaceTB/SmolLM2-135M\"   # tiny & quick\n",
        "# ALT examples:\n",
        "# BASE_MODEL = \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\"\n",
        "# BASE_MODEL = \"meta-llama/Llama-3.1-8B\"  # needs bigger GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0o4NAG8VKFw"
      },
      "source": [
        "#2) Use a small supervised dataset (chat/coding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "d4821b925c33488eb3a856b930442d83",
            "5da90b5cf24f4257b2ef86ed142c5f74",
            "8d5b358ac8cc49019386f56879699e63",
            "a5f3fb11599145a1b897e05ab36a51b9",
            "95af7ebd5fd744ac8d9c74fffde673f4",
            "3324e8cd1976449e8f02f40063ef6511",
            "debe8694efb94e8eb626abb0cd37d716",
            "5a353724ba75449bb31c7ba7a33f4dfe",
            "eb62565eff6b47558f111d11784d6c85",
            "6d7aa31de6a14d51b8ba7865bf96dc86",
            "4d4434b0534643ff8786360b5f604657",
            "d1adee3d435b416e92a679a6d287cb97",
            "8f91c90e42f647eaab92f27821c5234c",
            "6496e31c98184b37b62c9354397d358b",
            "c1f1a28d47184582a72a908db021ecae",
            "c846b1959b964a80b301144bddd415b0",
            "c86d8a22f7dd4811b90c066b2a07b445",
            "c45d29561b894b688e0c83717297d60e",
            "3f1dbaa20a794b20a76c2df2632b0513",
            "ebca9bf123304762ac924687b0bf0ef6",
            "2b8d4bbfa83543789dad12e8bfd6c6b4",
            "68ed6e1cc86e4ad3949fdc8f5061a8a3",
            "d95d1d97a34b45d08b40216a192a9fd1",
            "22eb182eff48461d90182029730335a6",
            "2cd4aff8c1184f49833773df52dd62bc",
            "5ff43e3628324df6b1bbf186554132a7",
            "064f55a6e2de46cf9f724a9d7f6f872c",
            "ae658e06031f426687a158bd4c8d8f9e",
            "02e916fe118b42e5b5c1660667b41332",
            "39a3b25e864c458584ea7091d4464de9",
            "562f1891469d4c4086456f24d0434e14",
            "e41848b57b1a4d5789c2789103220194",
            "fc60f1e7d75d4e8ea6ce261f94367784",
            "c712485427fe4823b5bfd86106e3acad",
            "7535f7a4358f4e5f86836045f69ee3c9",
            "9f05b8a74ae04755a00554fff1e2de30",
            "e2dfa624db0d44cfa3c011bf4d18b30d",
            "b3fd2c2370924e918f10d2fa3eb8960c",
            "0a93541bbaf548998b45745fd0ca804f",
            "01722685c1e94966b778f2b0603dbf6a",
            "a950200e870e4e258aed18ad51d0664b",
            "29458bc746bb489981c7f2b3ac93d0d5",
            "32f16ec3208844479ee03aec37f4e70a",
            "1755247b9c6241f6bc0e5331fe6fb127"
          ]
        },
        "id": "nyYPLkszVM9s",
        "outputId": "8bc5161e-20ca-426c-cf20-dc71d2e24f11"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4821b925c33488eb3a856b930442d83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1adee3d435b416e92a679a6d287cb97",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001-a09b74b3ef9c3b(â€¦):   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d95d1d97a34b45d08b40216a192a9fd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c712485427fe4823b5bfd86106e3acad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load a simpler instruction-following dataset\n",
        "ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:500]\")\n",
        "\n",
        "def to_chat(example):\n",
        "    # Format the Alpaca dataset into a chat-like structure\n",
        "    # This is a simplified conversion; a proper chat template would be better\n",
        "    messages = []\n",
        "    if example[\"instruction\"]:\n",
        "        messages.append({\"role\": \"user\", \"content\": example[\"instruction\"]})\n",
        "    if example[\"input\"]:\n",
        "         messages.append({\"role\": \"user\", \"content\": example[\"input\"]}) # Append input as part of user message or a separate turn if appropriate for the template\n",
        "    if example[\"output\"]:\n",
        "        messages.append({\"role\": \"assistant\", \"content\": example[\"output\"]})\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "# Apply the formatting function and remove original columns\n",
        "train = ds.map(to_chat, remove_columns=[\"instruction\", \"input\", \"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utLLFKOBVTz5"
      },
      "source": [
        "#3) Load model for full fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcX31HfcVW9_",
        "outputId": "afff8140-ef43-460e-9efc-36766c331313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Float16 full finetuning uses more memory since we upcast weights to float32.\n",
            "HuggingFaceTB/SmolLM2-135M does not have a padding token! Will use pad_token = <|endoftext|>.\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = 2048\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = BASE_MODEL,\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False,         # full precision training\n",
        "    load_in_8bit = False,\n",
        "    dtype = torch.float16,\n",
        "    full_finetuning = True,       # <-- key for FFT\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IYNvlSuVazB"
      },
      "source": [
        "#4) Pack data and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "f83ff45fad25443ebb14d84f588dadfe",
            "f46bc0279bc64209a402261f695e2e2d",
            "2eeb7feeda984943a1f885ed056ce473",
            "bc18eb0b166a452b8530bf025aaa4aff",
            "f08e992591ec403fb70de1fdd63dfcb1",
            "65df4bd567f24fce91d08205a1499ecf",
            "fc3b407a643048f9b2d7eab23b869a79",
            "4500f214b6f64dc299a01ce0d716857d",
            "423facf4d4474f3cb37637dd1b4e3ea2",
            "38780e7cf21d4fc0806fb9c9bf49d95d",
            "8c3ce6fb338a413e899de267f5d0c727",
            "ca9dc708a2e94f77aa1818fbbf58e983",
            "fc936027deac46dc9ad0e6824ec5abc9",
            "90e1b7031245476ab2480061275ae4ed",
            "5330b9fd9bfb4144a04f9de7521dcd6a",
            "0362dd8850384fa2a206d6010e2a2504",
            "b1711a4584744dc7a7cb533bee1afb75",
            "e2367430d51a4331b67cf40987c60793",
            "1a9a56a41836421aba336849ab959655",
            "9e35a676dfd245d4ad58b2a767729e57",
            "75377fc09f0a424c9825ae4683466794",
            "dcaf4901696c467e9501f6a669e029dc"
          ]
        },
        "id": "8b528764",
        "outputId": "a0604446-0ecf-4759-da3b-fb9cf757a8f9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f83ff45fad25443ebb14d84f588dadfe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca9dc708a2e94f77aa1818fbbf58e983",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokenized examples: 500\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 0) Pick ONE template that matches your model.\n",
        "# If youâ€™re using Qwen/ChatML-style formatting, use this:\n",
        "CHATML_TEMPLATE = \"\"\"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{'<|im_start|>assistant\\n'}}{% endif %}\"\"\"\n",
        "\n",
        "# If your model is Llama 3 Instruct, use this instead:\n",
        "# LLAMA3_TEMPLATE = \"\"\"{% for message in messages %}{% if message['role'] == 'system' %}{{'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n' + message['content'] + '<|eot_id|>'}}{% elif message['role'] == 'user' %}{{'<|start_header_id|>user<|end_header_id|>\\n' + message['content'] + '<|eot_id|>'}}{% elif message['role'] == 'assistant' %}{{'<|start_header_id|>assistant<|end_header_id|>\\n' + message['content'] + '<|eot_id|>'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{'<|start_header_id|>assistant<|end_header_id|>\\n'}}{% endif %}\"\"\"\n",
        "\n",
        "# If your model is Gemma Instruct, use this:\n",
        "# GEMMA_TEMPLATE = \"\"\"{% for message in messages %}{% if loop.first %}{{'<bos>'}}{% endif %}{{'<start_of_turn>' + message['role'] + '\\n' + message['content'] + '<end_of_turn>\\n'}}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\"\"\n",
        "\n",
        "# 1) SET the template once (choose the one that fits your model)\n",
        "tokenizer.chat_template = CHATML_TEMPLATE  # swap to LLAMA3_TEMPLATE or GEMMA_TEMPLATE if needed\n",
        "\n",
        "# (Optional) ensure special tokens exist if your tokenizer doesnâ€™t know them\n",
        "# This is safe even if they already exist; itâ€™s a no-op then.\n",
        "specials = {\"additional_special_tokens\": [\"<|im_start|>\", \"<|im_end|>\"]}\n",
        "try:\n",
        "    num_added = tokenizer.add_special_tokens(specials)\n",
        "    if num_added:\n",
        "        print(f\"Added {num_added} special tokens to tokenizer.\")\n",
        "        # If you have `model`, do: model.resize_token_embeddings(len(tokenizer))\n",
        "except Exception as _:\n",
        "    pass  # not critical; training can still proceed\n",
        "\n",
        "def _is_valid_messages(msgs):\n",
        "    if not isinstance(msgs, list) or not msgs:\n",
        "        return False\n",
        "    for m in msgs:\n",
        "        if not (isinstance(m, dict) and \"role\" in m and \"content\" in m and isinstance(m[\"content\"], str)):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def tokenize_chat(example):\n",
        "    msgs = example.get(\"messages\")\n",
        "    if not _is_valid_messages(msgs):\n",
        "        return {\"input_ids\": [], \"attention_mask\": []}\n",
        "\n",
        "    try:\n",
        "        # Render -> text (template comes from tokenizer.chat_template)\n",
        "        rendered = tokenizer.apply_chat_template(\n",
        "            msgs,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "        if not rendered.strip():\n",
        "            return {\"input_ids\": [], \"attention_mask\": []}\n",
        "\n",
        "        toks = tokenizer(rendered, add_special_tokens=False, return_attention_mask=True)\n",
        "        ids = toks.get(\"input_ids\", [])\n",
        "        attn = toks.get(\"attention_mask\", [])\n",
        "        return {\"input_ids\": ids, \"attention_mask\": attn} if ids else {\"input_ids\": [], \"attention_mask\": []}\n",
        "    except Exception as e:\n",
        "        print(\"Tokenization error:\", repr(e))\n",
        "        try:\n",
        "            print(\"First roles:\", [m.get(\"role\",\"?\") for m in msgs[:5]])\n",
        "        except:\n",
        "            pass\n",
        "        return {\"input_ids\": [], \"attention_mask\": []}\n",
        "\n",
        "# 2) Map â†’ return python lists (Arrow-friendly), then filter non-empty rows\n",
        "train_tokenized = train.map(\n",
        "    tokenize_chat,\n",
        "    remove_columns=[c for c in train.column_names if c != \"messages\"]  # keep messages for debug until after filter\n",
        ")\n",
        "\n",
        "train_tokenized = train_tokenized.filter(lambda x: isinstance(x[\"input_ids\"], list) and len(x[\"input_ids\"]) > 0)\n",
        "\n",
        "# 3) Now drop 'messages' if you donâ€™t need it further\n",
        "if \"messages\" in train_tokenized.column_names:\n",
        "    train_tokenized = train_tokenized.remove_columns([\"messages\"])\n",
        "\n",
        "print(f\"Number of tokenized examples: {len(train_tokenized)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a298d367"
      },
      "source": [
        "#5) Quick evaluation + inference demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd5KLRt7nMWp",
        "outputId": "bd849d14-c59e-43ee-bc1c-a24d7aebda24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'accuracy': 0.75}\n"
          ]
        }
      ],
      "source": [
        "# !pip install -q evaluate\n",
        "import evaluate\n",
        "\n",
        "# Example: accuracy\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "preds = [1, 0, 1, 1]\n",
        "refs  = [1, 0, 0, 1]\n",
        "print(metric.compute(predictions=preds, references=refs))  # {'accuracy': 0.75}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCFjw1czn1Hx",
        "outputId": "cb5a4b05-381e-47fa-bc5d-2939ba82a2cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "MODEL_ID = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "gen_kwargs = {\n",
        "    \"max_new_tokens\": 150,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "    \"repetition_penalty\": 1.2,\n",
        "    \"eos_token_id\": tokenizer.eos_token_id,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "    \"return_full_text\": False\n",
        "}\n",
        "\n",
        "def generate(prompt):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    out = pipe(text, **gen_kwargs)[0][\"generated_text\"]\n",
        "    return out.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEWQy5-4qxYm",
        "outputId": "c86a86c4-a296-4346-a168-dd70f053bf4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A Python decorator is an object that defines a set of methods or properties to be decorated for use within another class or module. It's used to add additional functionality and flexibility without modifying the original codebase. For instance:\n",
            "```python\n",
            "@decorator_name(method1) # This will decorate method2 as 'add' instead of just \"lambda\" (which would not work here).\n",
            "def myfunc():\n",
            "    pass\n",
            "myfunction = @wraparound(\"call\")()\n",
            "print(f\"{type(myfunction).__name__} called.\")\n",
            "# Outputs:\n",
            "// Calling `wrapper` function does nothing because it was already wrapped inside `functools`. However, if you want to call this wrapper directly like\n"
          ]
        }
      ],
      "source": [
        "print(generate(\n",
        "    \"Write a clean Python function to reverse a linked list iteratively. Only code.\"\n",
        "))\n",
        "print(\"----\")\n",
        "print(generate(\n",
        "    \"Explain what a Python decorator is in 3 sentences with a small example.\"\n",
        "))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}