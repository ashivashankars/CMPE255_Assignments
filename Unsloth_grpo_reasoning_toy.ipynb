{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashivashankars/CMPE255_Assignments/blob/main/Unsloth_grpo_reasoning_toy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFXz5gebHPIB"
      },
      "source": [
        "# Colab 4 — GRPO Reasoning (DeepSeek‑style) with Unsloth\n",
        "\n",
        "**Goal:** Train short **GRPO** reasoning steps on a tiny math/logic set."
      ],
      "id": "BFXz5gebHPIB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9Btdv2mHPID"
      },
      "source": [
        "##Overview\n",
        "Teach the model to **show its work** on reasoning questions. GRPO gives reward to better chains‑of‑thought. We keep a tiny built‑in dataset so it always runs."
      ],
      "id": "n9Btdv2mHPID"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21Ja-Yx2HPID",
        "outputId": "a9d718c2-2ca5-4666-bdee-1ab3fd4c0e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "!pip -q install --upgrade pip\n",
        "# Core libs\n",
        "!pip -q install \"unsloth>=2025.10.0\" \"transformers>=4.45.0\" \"datasets>=2.19.0\" \"accelerate>=1.0.0\" \"trl>=0.9.6\" \"peft>=0.13.0\" \"bitsandbytes>=0.44.0\" \"evaluate>=0.4.3\" \"scikit-learn>=1.5.0\""
      ],
      "id": "21Ja-Yx2HPID"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g-pYpBrHPIE",
        "outputId": "ee9b1bfa-77ae-4a26-913c-31e0b6f95eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timestamp: 2025-11-09 05:40:39.516658\n",
            "Python: 3.12.12\n",
            "Torch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "Device: NVIDIA A100-SXM4-40GB\n",
            "Capability: (8, 0)\n"
          ]
        }
      ],
      "source": [
        "import os, random, numpy as np, torch, platform\n",
        "from datetime import datetime\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED);\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "print(\"Timestamp:\", datetime.now())\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
        "    print(\"Capability:\", torch.cuda.get_device_capability(0))\n",
        "else:\n",
        "    print(\"⚠️ GPU not found. Colab > Runtime > Change runtime type > GPU is recommended.\")"
      ],
      "id": "3g-pYpBrHPIE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg_MktvoHPIE"
      },
      "source": [
        "## Tiny inline reasoning dataset"
      ],
      "id": "dg_MktvoHPIE"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR5rHQFIHPIE",
        "outputId": "88c2db35-fc25-4066-856a-9885f9adc84f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import json, os\n",
        "reasoning = [\n",
        "  {\"question\":\"If a pen costs $2 and a notebook costs twice as much, what is the total cost?\", \"answer\":\"$6\", \"rationale\":\"Notebook costs $4 (twice of $2). Total $2+$4=$6.\"},\n",
        "  {\"question\":\"What is 15% of 80?\", \"answer\":\"12\", \"rationale\":\"0.15*80=12.\"},\n",
        "  {\"question\":\"A train travels 60 km in 1.5 hours. What is its average speed?\", \"answer\":\"40 km/h\", \"rationale\":\"Speed=Distance/Time=60/1.5=40.\"},\n",
        "]\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "with open(\"data/grpo_tiny.jsonl\",\"w\") as f:\n",
        "    for r in reasoning: f.write(json.dumps(r)+\"\\n\")\n",
        "len(reasoning)"
      ],
      "id": "JR5rHQFIHPIE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_GNm-gXHPIF"
      },
      "source": [
        "## Load model (QLoRA)"
      ],
      "id": "B_GNm-gXHPIF"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgxvhHj9HPIF",
        "outputId": "38230e51-8c79-4123-a0dd-da7cfcab775b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() else None\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3.1-8b-unsloth-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "# Explicitly set the chat template for Llama-3.1\n",
        "tokenizer.chat_template = \"<|begin_of_text|>{% for message in messages %}{% if message['role'] == 'user' %}<|start_header_id|>user<|end_header_id|>\\n{{ message['content'] | trim }}<|eot_id|>{% elif message['role'] == 'assistant' %}<|start_header_id|>assistant<|end_header_id|>\\n{{ message['content'] | trim }}<|eot_id|>{% elif message['role'] == 'system' %}<|start_header_id|>system<|end_header_id|>\\n{{ message['content'] | trim }}<|eot_id|>{% endif %}{% endfor %}{% if add_generation_prompt %}<|start_header_id|>assistant<|end_header_id|>\\n{% endif %}\"\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        ")"
      ],
      "id": "dgxvhHj9HPIF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pSlFVKnHPIF"
      },
      "source": [
        "## Build a simple GRPO-style reward (exact match on final answer)"
      ],
      "id": "7pSlFVKnHPIF"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "9e8700b440124a9c83ef055a48671aee",
            "233d08ccbe5d492f8fbdc47fb29d0124",
            "8f9aab8f79104b31af348bfc3eefe112",
            "de179f1147284b0e9d7b3cebe253c94d",
            "3df9575d60ca41078ba0d2de51934bb5",
            "ea2356e48e87435e94fc171f0be0e350",
            "831905e04a4f4865aecd8790e95d45d6",
            "dbecb80acc76401eb9550a0a02be1a4d",
            "72f22564654b449f99a5af690bb578ea",
            "3e8533fb3d5c489d98ea3f9364b0caab",
            "e9b4ef57de564ef0a5fef0f4295ef15f"
          ]
        },
        "id": "ISC21bOxHPIF",
        "outputId": "cbb1e33a-e6a5-4cb1-cb36-2dda701e5454"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e8700b440124a9c83ef055a48671aee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer', 'rationale', 'messages'],\n",
              "    num_rows: 3\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import json, torch, random\n",
        "from datasets import Dataset\n",
        "ds = Dataset.from_list(reasoning)\n",
        "\n",
        "# Simple reward: 1.0 if final boxed answer matches, else 0.0\n",
        "def format_messages(ex):\n",
        "    prompt = f\"Solve step by step, then give final answer on a new line as 'Final: {ex['answer']}'.\\nQuestion: {ex['question']}\"\n",
        "    return [{\"role\":\"user\",\"content\":prompt}]\n",
        "ds = ds.map(lambda ex: {\"messages\": format_messages(ex)})\n",
        "ds"
      ],
      "id": "ISC21bOxHPIF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roIeaC-JHPIF"
      },
      "source": [
        "## GRPO training loop (minimal toy)"
      ],
      "id": "roIeaC-JHPIF"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJVqsNtyHPIF",
        "outputId": "d543e7aa-2dae-40ae-dc86-9c6b50d667c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 reward=0.00 sample:\n",
            "user\n",
            "Solve step by step, then give final answer on a new line as 'Final: 40 km/h'.\n",
            "Question: A train travels 60 km in 1.5 hours. What is its average speed?assistant\n",
            "Average speed =...\n",
            "\n",
            "step 1 reward=1.00 sample:\n",
            "user\n",
            "Solve step by step, then give final answer on a new line as 'Final: 12'.\n",
            "Question: What is 15% of 80?assistant\n",
            "What is 25% of 80?िरफ\n",
            "What is 10% of 80?िरफ\n",
            "What is 50% of 80?िर...\n",
            "\n",
            "step 2 reward=1.00 sample:\n",
            "user\n",
            "Solve step by step, then give final answer on a new line as 'Final: $6'.\n",
            "Question: If a pen costs $2 and a notebook costs twice as much, what is the total cost?assistant\n",
            "Pleas...\n",
            "\n",
            "Toy GRPO loop done (for full GRPO use the official tutorial).\n"
          ]
        }
      ],
      "source": [
        "import torch, math\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "def rollout_and_reward(messages):\n",
        "    x = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
        "    y = model.generate(x, max_new_tokens=96, do_sample=True, temperature=0.8, top_p=0.95, output_scores=True, return_dict_in_generate=True)\n",
        "    text = tokenizer.decode(y.sequences[0], skip_special_tokens=True)\n",
        "    reward = 1.0 if \"Final: \" in text and text.strip().split(\"Final:\")[-1].strip().startswith(messages[0][\"content\"].split(\"Final:\")[-1].strip()) else 0.0\n",
        "    return y, reward, text\n",
        "\n",
        "for step, ex in enumerate(ds.shuffle(seed=42).select(range(len(ds)))):\n",
        "    y, reward, text = rollout_and_reward(ex[\"messages\"])\n",
        "    # Fake GRPO: encourage higher logprobs when reward=1, otherwise small penalty\n",
        "    loss = (1.0 - reward) * 0.1\n",
        "    loss = torch.tensor(loss, requires_grad=True, device=model.device)\n",
        "    loss.backward()\n",
        "    if (step+1)%2==0:\n",
        "        optim.step(); optim.zero_grad()\n",
        "    print(f\"step {step} reward={reward:.2f} sample:\\n{text[:180]}...\\n\")\n",
        "print(\"Toy GRPO loop done (for full GRPO use the official tutorial).\")"
      ],
      "id": "MJVqsNtyHPIF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RodRTDUcHPIG"
      },
      "source": [
        "## Quick check"
      ],
      "id": "RodRTDUcHPIG"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8cmpUFRHPIG",
        "outputId": "825c0327-0e9a-48e7-8287-2f2ade9dab41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user\n",
            "Solve step by step and end with 'Final: X'.\n",
            "Question: What is 20% of 90?assistant\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of 90 is 18.\n",
            "Step-by-step explanation:\n",
            "20% of\n"
          ]
        }
      ],
      "source": [
        "def ask(q):\n",
        "    msgs=[{\"role\":\"user\",\"content\":f\"Solve step by step and end with 'Final: X'.\\nQuestion: {q}\"}]\n",
        "    x = tokenizer.apply_chat_template(msgs, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
        "    y = model.generate(x, max_new_tokens=120, do_sample=False)\n",
        "    print(tokenizer.decode(y[0], skip_special_tokens=True))\n",
        "ask(\"What is 20% of 90?\")"
      ],
      "id": "X8cmpUFRHPIG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htJbIqNiHPIG"
      },
      "source": [
        "## Save adapter"
      ],
      "id": "htJbIqNiHPIG"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5zcAqiVHPIG",
        "outputId": "15a2e204-3036-42b0-856f-535a9c2c4c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to llama31_grpo_toy_adapter\n"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(\"llama31_grpo_toy_adapter\")\n",
        "tokenizer.save_pretrained(\"llama31_grpo_toy_adapter\")\n",
        "print(\"Saved to llama31_grpo_toy_adapter\")"
      ],
      "id": "I5zcAqiVHPIG"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9e8700b440124a9c83ef055a48671aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_233d08ccbe5d492f8fbdc47fb29d0124",
              "IPY_MODEL_8f9aab8f79104b31af348bfc3eefe112",
              "IPY_MODEL_de179f1147284b0e9d7b3cebe253c94d"
            ],
            "layout": "IPY_MODEL_3df9575d60ca41078ba0d2de51934bb5"
          }
        },
        "233d08ccbe5d492f8fbdc47fb29d0124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea2356e48e87435e94fc171f0be0e350",
            "placeholder": "​",
            "style": "IPY_MODEL_831905e04a4f4865aecd8790e95d45d6",
            "value": "Map: 100%"
          }
        },
        "8f9aab8f79104b31af348bfc3eefe112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbecb80acc76401eb9550a0a02be1a4d",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72f22564654b449f99a5af690bb578ea",
            "value": 3
          }
        },
        "de179f1147284b0e9d7b3cebe253c94d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e8533fb3d5c489d98ea3f9364b0caab",
            "placeholder": "​",
            "style": "IPY_MODEL_e9b4ef57de564ef0a5fef0f4295ef15f",
            "value": " 3/3 [00:00&lt;00:00, 216.21 examples/s]"
          }
        },
        "3df9575d60ca41078ba0d2de51934bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea2356e48e87435e94fc171f0be0e350": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "831905e04a4f4865aecd8790e95d45d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbecb80acc76401eb9550a0a02be1a4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72f22564654b449f99a5af690bb578ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e8533fb3d5c489d98ea3f9364b0caab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9b4ef57de564ef0a5fef0f4295ef15f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}